{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learn_tweets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lazy0107/nlp-tweet/blob/master/learn_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sfTHfbNjNl5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n\n",
        "!sed -e \"s!/var/lib/mecab/dic/debian!/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd!g\" /etc/mecabrc > /etc/mecabrc.new\n",
        "!cp /etc/mecabrc /etc/mecabrc.old\n",
        "!cp /etc/mecabrc.new /etc/mecabrc\n",
        "!apt-get -q -y install swig\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QCbRKhmrQ_bf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pickle\n",
        "from datetime import datetime as dt\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "import MeCab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A5wmvYN1DUuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets_to_learn = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M_dfk-tJHcto",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list_tweets = []\n",
        "for key in tweets_to_learn.keys():\n",
        "  f = open(key, 'rb')\n",
        "  list_tweets.append(pickle.load(f))\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-SgsqPv5wPSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list_tweets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TyR2_EEBJkbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for list_tweet in list_tweets:\n",
        "  text = ''.join(list_tweet)\n",
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RiTQdt0pgrH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#chars = sorted(list(set(text)))\n",
        "#print('total chars:', len(chars))\n",
        "#char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "#indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "#maxlen = 5\n",
        "#step = 1\n",
        "#sentences = []\n",
        "#next_chars = []\n",
        "#for i in range(0, len(text) - maxlen, step):\n",
        "#  sentences.append(text[i: i + maxlen])\n",
        "#  next_chars.append(text[i + maxlen])\n",
        "#print('nb sequences:', len(sentences))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3RS1z09G2RH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#chars = sorted(list(set(text)))\n",
        "#print('total chars:', len(chars))\n",
        "#char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "#indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "#文字→単語\n",
        "#text =Tokenizer().tokenize(text, wakati=True)  # 分かち書きする\n",
        "mecab = MeCab.Tagger(\"-Owakati\")\n",
        "text = mecab.parse(text)\n",
        "text = text.split()\n",
        "#chars = text\n",
        "chars = sorted(list(set(text)))\n",
        "count = 0\n",
        "char_indices = {}  # 辞書初期化\n",
        "indices_char = {}  # 逆引き辞書初期化\n",
        " \n",
        "for word in chars:\n",
        "  if not word in char_indices:  # 未登録なら\n",
        "    char_indices[word] = count # 登録する      \n",
        "    count +=1\n",
        "    print(count,word)  # 登録した単語を表示\n",
        "# 逆引き辞書を辞書から作成する\n",
        "indices_char = dict([(value, key) for (key, value) in char_indices.items()])\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 5\n",
        "step = 1\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "  sentences.append(text[i: i + maxlen])\n",
        "  next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lyMi3OEqhqzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(len(sentences), maxlen, len(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dRYTzuNcn44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ejaAY6uLeYPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_indices[char]] = 1\n",
        "  y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5RD_mA7RehF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build the model: a single LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "26DCcFx9RoHG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gfc_sFKUMscO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "  # Function invoked at end of each epoch. Prints generated text.\n",
        "  print()\n",
        "  print('----- Generating text after Epoch: %d' % epoch)\n",
        " \n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "#  for diversity in [0.2]:\n",
        "    print('----- diversity:', diversity)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "#    generated += sentence\n",
        "    # sentence はリストなので文字列へ変換して使用\n",
        "    generated += ''.join(sentence)\n",
        "    \n",
        "    print('----- Generating with seed: \"' + ''.join(sentence) + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "\n",
        "#    for i in range(400):\n",
        "    for i in range(140-maxlen):\n",
        "      x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(sentence):\n",
        "        x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "      preds = model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample(preds, diversity)\n",
        "      next_char = indices_char[next_index]\n",
        "      # 140字以内になるように制御\n",
        "      if len(generated) + len(next_char) > 140:\n",
        "        break\n",
        "      generated += next_char\n",
        "#      sentence = sentence[1:] + next_char\n",
        "      sentence = sentence[1:]\n",
        "      # sentence はリストなので append で結合する\n",
        "      sentence.append(next_char)\n",
        "\n",
        "      sys.stdout.write(next_char)\n",
        "      sys.stdout.flush()\n",
        "    list_generated.append(generated)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sOtZM1uKNY6r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list_generated = []\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=60,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHLjBjVHQ1V0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list_generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQVtq2N8EUws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tdatetime = dt.now()\n",
        "str_ymd = tdatetime.strftime('%Y%m%d%H%M%S')\n",
        "filename = str_ymd + '_textGenerated.pickle'\n",
        "with open(filename, 'wb') as f:\n",
        "  pickle.dump(list_generated, f)\n",
        "files.download(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}